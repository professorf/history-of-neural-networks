# history-of-neural-networks
The history of neural networks from McCulloch and Pitts to Transformers and Large Language Models, with a focus on architecture.

# Motivation
As a cognitive scientist, I write research papers on neural networks applications for an interdiscipinary audience. Some readers are intimately familiar with the history and mathematics of neural networks, while many others, particulary in the non-technical social sciences, only understand them from a functional an experiential level â€• from using publicly available AI chat bots like ChatGPT.  

Every time I write a new research paper for a new audience I find my self rewriting the history section and struggling with how much detail to put into the section. Through trial and error, the best approach I've found to writing a brief history of neural networks for a diverse audience is to start with McCulloch and Pitts (1943) model of neurons, to trace the architectures that have evolved, and to note the applications of those architectures. The idea is to start with  the perceptrons of Rosenblatt (1958), to the feed-forward networks with a single hidden layer from the UCSD Parallel-Distributed Processing group (Rumelhart, Hinton, and Williams, 1985), to the multiple-hidden layer, deep learning network of Hinton, Osindero, and Teh (2006), and ending with the transformers of Vaswani et al (2017).

# References

Hinton, G., Osindero, S., & Teh, Y. (2006). A fast learning algorithm for deep belief nets. _Neural computation_, _18_, 1527-1554.

McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. _The bulletin of mathematical biophysics_, _5_, 115-133.

Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. _Psychological review_, _65_, 386-408.

Rumelhart, D., Hinton, G., Williams, R. (1986). Learning internal representations by error propagation. In D. Rumelhart & J. McClelland (Eds.), _Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Vol. 1: Foundations_ (pp. 318-362) .
Cambridge, MA: Bradford Books/MIT Press.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. , ... & Polosukhin, I. (2017). Attention is all you need. _Advances in neural information processing systems 30, 31st Conference on Neural Information Processing Systems (NIPS 2017)_, Long Beach, CA.

